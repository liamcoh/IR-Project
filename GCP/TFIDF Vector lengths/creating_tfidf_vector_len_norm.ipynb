{"cells":[{"cell_type":"code","execution_count":2,"id":"a3c19967","metadata":{"id":"a3c19967","outputId":"1d7d876f-730c-4f50-9e6f-3b5a0dd9f71f"},"outputs":[{"name":"stdout","output_type":"stream","text":["NAME          PLATFORM  WORKER_COUNT  PREEMPTIBLE_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n","cluster-862a  GCE       4                                       RUNNING  us-central1-a\r\n"]}],"source":["!gcloud dataproc clusters list --region us-central1\n"]},{"cell_type":"code","execution_count":3,"id":"1585f8a0","metadata":{"id":"1585f8a0","outputId":"93d3f9b5-c6d0-4987-b01c-99a08573d744"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\n"]}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes"]},{"cell_type":"code","execution_count":4,"id":"11fc4ced","metadata":{"id":"11fc4ced","outputId":"1e2ec591-9f55-4315-b135-e60d6bb1226d"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\n","Package openjdk-8-jdk-headless is not available, but is referred to by another package.\n","This may mean that the package is missing, has been obsoleted, or\n","is only available from another source\n","\n","E: Package 'openjdk-8-jdk-headless' has no installation candidate\n"]}],"source":["!pip install -q pyspark\n","!pip install -U -q PyDrive\n","!apt-get update -qq\n","!apt install openjdk-8-jdk-headless -qq"]},{"cell_type":"code","execution_count":5,"id":"0cadb09e","metadata":{"id":"0cadb09e","outputId":"f00f92dd-0198-491b-aedb-3008df8e8846"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import math\n","import pandas as pd\n","from google.cloud import storage\n","\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":null,"id":"73de29f5","metadata":{"id":"73de29f5"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":6,"id":"86624e8c","metadata":{"id":"86624e8c","outputId":"c6f78614-608e-490a-ce32-bb515ac3bb07"},"outputs":[{"name":"stdout","output_type":"stream","text":["-rw-r--r-- 1 root root 247882 Jan  5 11:01 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"]}],"source":["!ls -l /usr/lib/spark/jars/graph*"]},{"cell_type":"code","execution_count":7,"id":"1a389b7c","metadata":{"id":"1a389b7c"},"outputs":[],"source":["from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *"]},{"cell_type":"code","execution_count":8,"id":"a9946102","metadata":{"id":"a9946102"},"outputs":[],"source":["# Put your bucket name below and make sure you can access it without an error\n","bucket_name = '316399401' \n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)"]},{"cell_type":"code","execution_count":9,"id":"654d3b46","metadata":{"id":"654d3b46","outputId":"8918cf7c-1ee2-48f9-c2e0-83b7b70d1e4e"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["full_path = \"gs://wikidata_preprocessed/*\"\n","parquetFile = spark.read.parquet(full_path)\n","doc_text_pairs = parquetFile.select(\"text\",\"title\",\"id\").rdd\n"]},{"cell_type":"code","execution_count":44,"id":"dbba645e","metadata":{"id":"dbba645e"},"outputs":[],"source":["english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","# NUM_BUCKETS = 124\n","# def token2bucket_id(token):\n","#   return int(_hash(token),16) % NUM_BUCKETS\n","\n","def word_count(text, id):\n","  ''' Count the frequency of each word in `text` (tf) that is not included in \n","  `all_stopwords` and return entries that will go into our posting lists. \n","  Parameters:\n","  -----------\n","    text: str\n","      Text of one document\n","    id: int\n","      Document id\n","  Returns:\n","  --------\n","    List of tuples\n","      A list of (token, (doc_id, tf)) pairs \n","      for example: [(\"Anarchism\", (12, 5)), ...]\n","  '''\n","  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","  words = [token for token in tokens if token not in all_stopwords]\n","  counter = Counter()\n","  counter.update(words)\n","  final = [(token,(id,counter[token])) for token in counter.keys()]\n","  # raise NotImplementedError()\n","  return final\n","\n","def calc_tfidf(record):\n","    print(\"a\")\n","    if (len(record)==0):\n","        return\n","    N=len(DL)\n","    sumArr = 0\n","    print(record)\n","    docid=record[0][1][0]\n","    for rec in record:\n","        print(rec)\n","        term = rec[0]\n","        print(term)\n","        tf = rec[1][1]/DL[docid]\n","        print(tf)\n","        if term not in w2df_dict.keys():\n","            continue\n","        df = w2df_dict[term]\n","        tfidf = tf*math.log(N/df,10)\n","        tfidfSquared = tfidf**2\n","        sumArr = sumArr + tfidfSquared\n","    return (docid,math.sqrt(sumArr))\n","  \n","\n","def reduce_word_counts(unsorted_pl):\n","  ''' Returns a sorted posting list by wiki_id.\n","  Parameters:\n","  -----------\n","    unsorted_pl: list of tuples\n","      A list of (wiki_id, tf) tuples \n","  Returns:\n","  --------\n","    list of tuples\n","      A sorted posting list.\n","  '''\n","  # YOUR CODE HERE\n","  return sorted(unsorted_pl,key=lambda x: x[0])\n","\n","def calculate_df(postings):\n","  ''' Takes a posting list RDD and calculate the df for each token.\n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each element is a (token, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each element is a (token, df) pair.\n","  '''\n","  # YOUR CODE HERE\n","  # raise NotImplementedError()\n","  return postings.map(lambda x: (x[0],len(x[1])))\n","\n"]},{"cell_type":"code","execution_count":17,"id":"Q3VNXoxSqdHT","metadata":{"id":"Q3VNXoxSqdHT"},"outputs":[],"source":["# def max_freq(text):\n","#   tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","#   words = [token for token in tokens if token not in all_stopwords]\n","#   counter = Counter()\n","#   counter.update(words)\n","#   return counter.most_common()[0]   \n"]},{"cell_type":"code","execution_count":11,"id":"eM_QpUbNznWB","metadata":{"id":"eM_QpUbNznWB"},"outputs":[],"source":["def tokenize(text):\n","  #TODO probablly need some fixes\n","    \"\"\"\n","    This function aims in tokenize a text into a list of tokens. Moreover, it filter stopwords.\n","    \n","    Parameters:\n","    -----------\n","    text: string , represting the text to tokenize.    \n","    \n","    Returns:\n","    -----------\n","    list of tokens (e.g., list of tokens).\n","    \"\"\"\n","    list_of_tokens =  [token.group() for token in RE_WORD.finditer(text.lower()) if token.group() not in all_stopwords]    \n","    return list_of_tokens"]},{"cell_type":"code","execution_count":12,"id":"z3goxThTzmkT","metadata":{"id":"z3goxThTzmkT"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["doc_wordlist_pairs = doc_text_pairs.map(lambda x: (x[2],len(tokenize(x[0]))+len(tokenize(x[1]))))\n","\n","DL = doc_wordlist_pairs.collectAsMap()"]},{"cell_type":"code","execution_count":26,"id":"4690213b","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["6348910\n"]}],"source":["print(len(DL))"]},{"cell_type":"code","execution_count":null,"id":"3b5503d2","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# word counts map\n","word_counts = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[2]))\n","# docid_maxfreq_pairs = doc_text_pairs.map(lambda x: (x[2],max_freq(x[0])))\n","# docid_maxfreq_dict = docid_maxfreq_pairs.collectAsMap()\n","postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","# filtering postings and calculate df\n","postings_filtered = postings.filter(lambda x: len(x[1])>50)\n","w2df = calculate_df(postings_filtered)\n","w2df_dict = w2df.collectAsMap()\n","doc_word_counts = doc_text_pairs.map(lambda x: word_count(x[0], x[2]))\n"]},{"cell_type":"code","execution_count":29,"id":"984828ba","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["PythonRDD[32] at RDD at PythonRDD.scala:53\n"]}],"source":["print(doc_word_counts)"]},{"cell_type":"code","execution_count":null,"id":"FG8fE4t2qQgr","metadata":{"id":"FG8fE4t2qQgr"},"outputs":[{"name":"stderr","output_type":"stream","text":["22/01/05 16:39:40 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for broadcast_32_python !\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["6347138\n"]}],"source":["docid_veclen_pairs = doc_word_counts.filter(lambda x: len(x)!=0)\n","docid_veclen_pairs = docid_veclen_pairs.map(lambda x: calc_tfidf(x))\n","docid_veclen_dict = docid_veclen_pairs.collectAsMap()\n","print(len(docid_veclen_dict))\n","# partition posting lists and write out\n"]},{"cell_type":"code","execution_count":47,"id":"6c5bc69d","metadata":{},"outputs":[],"source":["#euclidian distance\n","pv_clean = 'tfidf_vec_len.pkl'\n","with open(pv_clean, 'wb') as f:\n","    pickle.dump(docid_veclen_dict, f)\n","\n","bucket = client.bucket(bucket_name)\n","blob = bucket.blob(f\"tfidf_vec_len/{pv_clean}\")\n","\n","blob.upload_from_filename(pv_clean)"]},{"cell_type":"code","execution_count":50,"id":"84d7aff9","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["617.8538153793296\n"]}],"source":[]},{"cell_type":"code","execution_count":null,"id":"00aaa625","metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"creating_DL_DT.ipynb","provenance":[]},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":5}