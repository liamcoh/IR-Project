{"cells":[{"cell_type":"markdown","id":"84e52e66","metadata":{"id":"84e52e66"},"source":["***Important*** DO NOT CLEAR THE OUTPUT OF THIS NOTEBOOK AFTER EXECUTION!!!"]},{"cell_type":"code","execution_count":1,"id":"e7d6212f","metadata":{"id":"e7d6212f","nbgrader":{"grade":false,"grade_id":"cell-Worker_Count","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"cf88b954-f39a-412a-d87e-660833e735b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["NAME          PLATFORM  WORKER_COUNT  PREEMPTIBLE_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n","cluster-9376  GCE       4                                       RUNNING  us-central1-a\r\n"]}],"source":["# if the following command generates an error, you probably didn't enable \n","# the cluster security option \"Allow API access to all Google Cloud services\"\n","# under Manage Security â†’ Project Access when setting up the cluster\n","!gcloud dataproc clusters list --region us-central1"]},{"cell_type":"markdown","id":"3b9bdfd5","metadata":{"id":"3b9bdfd5"},"source":["# Imports & Setup"]},{"cell_type":"code","execution_count":2,"id":"6839f730","metadata":{"id":"6839f730","nbgrader":{"grade":false,"grade_id":"cell-Setup","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"fc0e315d-21e9-411d-d69c-5b97e4e5d629","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\n"]}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes"]},{"cell_type":"code","execution_count":3,"id":"976b3404","metadata":{"id":"976b3404","nbgrader":{"grade":false,"grade_id":"cell-Imports","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"a24aa24b-aa75-4823-83ca-1d7deef0f0de"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from nltk import ngrams\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":4,"id":"e46f9c33","metadata":{"id":"e46f9c33","nbgrader":{"grade":false,"grade_id":"cell-jar","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"8f93a7ec-71e0-49c1-fc81-9af385849a90"},"outputs":[{"name":"stdout","output_type":"stream","text":["-rw-r--r-- 1 root root 247882 Jan  9 15:40 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"]}],"source":["# if nothing prints here you forgot to include the initialization script when starting the cluster\n","!ls -l /usr/lib/spark/jars/graph*"]},{"cell_type":"code","execution_count":5,"id":"ba419b7d","metadata":{"id":"ba419b7d","nbgrader":{"grade":false,"grade_id":"cell-pyspark-import","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *"]},{"cell_type":"code","execution_count":6,"id":"39678e4e","metadata":{"id":"39678e4e","nbgrader":{"grade":false,"grade_id":"cell-spark-version","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"07b4e22b-a252-42fb-fe46-d9050e4e7ca8","scrolled":true},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://cluster-9376-m.c.my-project1-334314.internal:38653\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.1.2</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>yarn</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>PySparkShell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7f2f50fe21c0>"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["spark"]},{"cell_type":"code","execution_count":7,"id":"0bfdf8e8","metadata":{"id":"0bfdf8e8","nbgrader":{"grade":false,"grade_id":"cell-bucket_name","locked":false,"schema_version":3,"solution":true,"task":false},"outputId":"515a4cbd-3d30-43c0-a4d8-997dbfae58cf"},"outputs":[],"source":["# Put your bucket name below and make sure you can access it without an error\n","bucket_name = '316399401' \n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n"]},{"cell_type":"markdown","id":"e44d01fc","metadata":{"id":"e44d01fc"},"source":["***GCP setup is complete!*** If you got here without any errors you've earned 10 out of the 35 points of this part."]},{"cell_type":"markdown","id":"bb11f137","metadata":{"id":"bb11f137"},"source":["# Building an inverted index"]},{"cell_type":"markdown","id":"8707255a","metadata":{"id":"8707255a"},"source":["Here, we read the entire corpus to an rdd, directly from Google Storage Bucket and use your code from Colab to construct an inverted index."]},{"cell_type":"code","execution_count":8,"id":"ccb0ddc4","metadata":{"id":"ccb0ddc4","outputId":"265921e4-c71d-4f1c-b513-381fcb055b60","scrolled":false},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["full_path = \"gs://wikidata_preprocessed/*\"\n","parquetFile = spark.read.parquet(full_path)\n","doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd"]},{"cell_type":"markdown","id":"69020963","metadata":{"id":"69020963"},"source":["We will count the number of pages to make sure we are looking at the entire corpus. The number of pages should be more than 6M"]},{"cell_type":"markdown","id":"58c3ca9b","metadata":{"id":"58c3ca9b"},"source":["Let's import the inverted index module. Note that you need to use the staff-provided version called `inverted_index_gcp.py`, which contains helper functions to writing and reading the posting files similar to the Colab version, but with writing done to a Google Cloud Storage bucket."]},{"cell_type":"code","execution_count":9,"id":"57249cf6","metadata":{"id":"57249cf6","outputId":"327fe81b-80f4-4b3a-8894-e74720d92e35"},"outputs":[{"name":"stdout","output_type":"stream","text":["inverted_index_gcp.py\r\n"]}],"source":["# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n","%cd -q /home/dataproc\n","!ls inverted_index_gcp.py"]},{"cell_type":"code","execution_count":10,"id":"5eef3d4d","metadata":{"id":"5eef3d4d","outputId":"2be26b10-7708-47ef-dff1-e492bcbedfaf","scrolled":true},"outputs":[],"source":["# adding our python module to the cluster\n","sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())"]},{"cell_type":"code","execution_count":11,"id":"d69ff76d","metadata":{"id":"d69ff76d"},"outputs":[],"source":["from inverted_index_gcp import InvertedIndex"]},{"cell_type":"markdown","id":"27aefef5","metadata":{"id":"27aefef5"},"source":["**YOUR TASK (10 POINTS)**: Use your implementation of `word_count`, `reduce_word_counts`, `calculate_df`, and `partition_postings_and_write` functions from Colab to build an inverted index for all of English Wikipedia in under 2 hours.\n","\n","A few notes: \n","1. The number of corpus stopwords below is a bit bigger than the colab version since we are working on the whole corpus and not just on one file.\n","2. You need to slightly modify your implementation of  `partition_postings_and_write` because the signature of `InvertedIndex.write_a_posting_list` has changed and now includes an additional argument called `bucket_name` for the target bucket. See the module for more details.\n","3. You are not allowed to change any of the code not coming from Colab. "]},{"cell_type":"code","execution_count":18,"id":"0fa8528e","metadata":{"id":"0fa8528e","nbgrader":{"grade":false,"grade_id":"cell-token2bucket","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","stemmer = PorterStemmer()\n","NUM_BUCKETS = 124\n","def token2bucket_id(token):\n","    return int(_hash(token),16) % NUM_BUCKETS\n","\n","def word_count(text, id):\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    tokens = [stemmer.stem(t) for t  in tokens if t not in all_stopwords]\n","    tokens = ngrams(tokens,2)\n","    fin_tokens=[]\n","    for tup in tokens:\n","        fin_tokens.append(tup[0]+\" \"+tup[1])\n","\n","    lst = []\n","    c = Counter(fin_tokens)\n","\n","    for t in c:\n","        lst.append((t, (id, c[t])))\n","\n","    return lst\n","\n","def reduce_word_counts(unsorted_pl):  \n","    sortedLst = sorted(unsorted_pl, key=lambda tup: tup[1])\n","\n","    return sortedLst\n","\n","def calculate_df(postings):\n","    return postings.mapValues(lambda x: len(x))\n","\n","def partition_postings_and_write(postings):  \n","    b_w_pl_lst = defaultdict(list)\n","    rdd_by_bucket = postings.map(lambda x: (token2bucket_id(x[0]),x)).groupByKey().mapValues(list)\n","    res_rdd = rdd_by_bucket.map(lambda x: InvertedIndex.write_a_posting_list(x, bucket_name))\n","    return res_rdd"]},{"cell_type":"code","execution_count":null,"id":"4fb579a5","metadata":{"id":"4fb579a5","nbgrader":{"grade":false,"grade_id":"cell-index_construction","locked":false,"schema_version":3,"solution":true,"task":false},"outputId":"e075a0c4-f525-4b7a-ab37-23df9195974a"},"outputs":[{"name":"stderr","output_type":"stream","text":["Exception in thread \"serve RDD 25\" java.net.SocketTimeoutException: Accept timed out\n","\tat java.net.PlainSocketImpl.socketAccept(Native Method)\n","\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\n","\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\n","\tat java.net.ServerSocket.accept(ServerSocket.java:528)\n","\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:64)\n","22/01/09 21:10:44 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 8.0 in stage 14.0 (TID 891) (cluster-9376-w-0.c.my-project1-334314.internal executor 25): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n","    out_iter = func(split_index, iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 418, in func\n","    return f(iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2234, in combine\n","    merger.mergeValues(iterator)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n","    for k, v in iterator:\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_8526/219271945.py\", line 37, in <lambda>\n","  File \"/tmp/ipykernel_8526/219271945.py\", line 12, in token2bucket_id\n","  File \"/tmp/ipykernel_8526/1607138781.py\", line 22, in _hash\n","TypeError: encoding without a string argument\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n","\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:748)\n","\n","22/01/09 21:10:54 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2.0 in stage 14.0 (TID 885) (cluster-9376-w-3.c.my-project1-334314.internal executor 28): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n","    out_iter = func(split_index, iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 418, in func\n","    return f(iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2234, in combine\n","    merger.mergeValues(iterator)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n","    for k, v in iterator:\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_8526/219271945.py\", line 37, in <lambda>\n","  File \"/tmp/ipykernel_8526/219271945.py\", line 12, in token2bucket_id\n","  File \"/tmp/ipykernel_8526/1607138781.py\", line 22, in _hash\n","TypeError: encoding without a string argument\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n","\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:748)\n","\n","22/01/09 21:11:06 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 14.0 (TID 883) (cluster-9376-w-2.c.my-project1-334314.internal executor 30): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n","    out_iter = func(split_index, iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 418, in func\n","    return f(iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2234, in combine\n","    merger.mergeValues(iterator)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n","    for k, v in iterator:\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_8526/219271945.py\", line 37, in <lambda>\n","  File \"/tmp/ipykernel_8526/219271945.py\", line 12, in token2bucket_id\n","  File \"/tmp/ipykernel_8526/1607138781.py\", line 22, in _hash\n","TypeError: encoding without a string argument\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n","\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:748)\n","\n","22/01/09 21:11:19 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 14.0 in stage 14.0 (TID 897) (cluster-9376-w-2.c.my-project1-334314.internal executor 34): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n","    out_iter = func(split_index, iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 418, in func\n","    return f(iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2234, in combine\n","    merger.mergeValues(iterator)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n","    for k, v in iterator:\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_8526/219271945.py\", line 37, in <lambda>\n","  File \"/tmp/ipykernel_8526/219271945.py\", line 12, in token2bucket_id\n","  File \"/tmp/ipykernel_8526/1607138781.py\", line 22, in _hash\n","TypeError: encoding without a string argument\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n","\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:748)\n","\n","22/01/09 21:12:18 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.1 in stage 14.0 (TID 901) (cluster-9376-w-0.c.my-project1-334314.internal executor 25): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n","    out_iter = func(split_index, iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 418, in func\n","    return f(iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2234, in combine\n","    merger.mergeValues(iterator)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n","    for k, v in iterator:\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_8526/219271945.py\", line 37, in <lambda>\n","  File \"/tmp/ipykernel_8526/219271945.py\", line 12, in token2bucket_id\n","  File \"/tmp/ipykernel_8526/1607138781.py\", line 22, in _hash\n","TypeError: encoding without a string argument\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n","\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:748)\n","\n","22/01/09 21:12:28 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 9.1 in stage 14.0 (TID 905) (cluster-9376-w-3.c.my-project1-334314.internal executor 27): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n","    out_iter = func(split_index, iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 418, in func\n","    return f(iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2234, in combine\n","    merger.mergeValues(iterator)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n","    for k, v in iterator:\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_8526/219271945.py\", line 37, in <lambda>\n","  File \"/tmp/ipykernel_8526/219271945.py\", line 12, in token2bucket_id\n","  File \"/tmp/ipykernel_8526/1607138781.py\", line 22, in _hash\n","TypeError: encoding without a string argument\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n","\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:748)\n","\n","22/01/09 21:12:42 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 12.1 in stage 14.0 (TID 912) (cluster-9376-w-2.c.my-project1-334314.internal executor 34): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n","    out_iter = func(split_index, iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 418, in func\n","    return f(iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2234, in combine\n","    merger.mergeValues(iterator)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n","    for k, v in iterator:\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_8526/219271945.py\", line 37, in <lambda>\n","  File \"/tmp/ipykernel_8526/219271945.py\", line 12, in token2bucket_id\n","  File \"/tmp/ipykernel_8526/1607138781.py\", line 22, in _hash\n","TypeError: encoding without a string argument\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n","\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:748)\n","\n","22/01/09 21:12:58 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.1 in stage 14.0 (TID 914) (cluster-9376-w-2.c.my-project1-334314.internal executor 34): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n","    out_iter = func(split_index, iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 418, in func\n","    return f(iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2234, in combine\n","    merger.mergeValues(iterator)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n","    for k, v in iterator:\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_8526/219271945.py\", line 37, in <lambda>\n","  File \"/tmp/ipykernel_8526/219271945.py\", line 12, in token2bucket_id\n","  File \"/tmp/ipykernel_8526/1607138781.py\", line 22, in _hash\n","TypeError: encoding without a string argument\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n","\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:748)\n","\n","22/01/09 21:13:50 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.2 in stage 14.0 (TID 916) (cluster-9376-w-0.c.my-project1-334314.internal executor 25): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n","    out_iter = func(split_index, iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 418, in func\n","    return f(iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2234, in combine\n","    merger.mergeValues(iterator)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n","    for k, v in iterator:\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_8526/219271945.py\", line 37, in <lambda>\n","  File \"/tmp/ipykernel_8526/219271945.py\", line 12, in token2bucket_id\n","  File \"/tmp/ipykernel_8526/1607138781.py\", line 22, in _hash\n","TypeError: encoding without a string argument\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n","\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:748)\n","\n","22/01/09 21:14:01 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 5.2 in stage 14.0 (TID 924) (cluster-9376-w-3.c.my-project1-334314.internal executor 27): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n","    out_iter = func(split_index, iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 418, in func\n","    return f(iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2234, in combine\n","    merger.mergeValues(iterator)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n","    for k, v in iterator:\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_8526/219271945.py\", line 37, in <lambda>\n","  File \"/tmp/ipykernel_8526/219271945.py\", line 12, in token2bucket_id\n","  File \"/tmp/ipykernel_8526/1607138781.py\", line 22, in _hash\n","TypeError: encoding without a string argument\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n","\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:748)\n","\n","22/01/09 21:14:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 6.2 in stage 14.0 (TID 928) (cluster-9376-w-2.c.my-project1-334314.internal executor 34): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n","    out_iter = func(split_index, iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 418, in func\n","    return f(iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2234, in combine\n","    merger.mergeValues(iterator)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n","    for k, v in iterator:\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_8526/219271945.py\", line 37, in <lambda>\n","  File \"/tmp/ipykernel_8526/219271945.py\", line 12, in token2bucket_id\n","  File \"/tmp/ipykernel_8526/1607138781.py\", line 22, in _hash\n","TypeError: encoding without a string argument\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n","\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:748)\n","\n","22/01/09 21:14:31 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 12.2 in stage 14.0 (TID 929) (cluster-9376-w-2.c.my-project1-334314.internal executor 30): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n","    out_iter = func(split_index, iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 418, in func\n","    return f(iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2234, in combine\n","    merger.mergeValues(iterator)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n","    for k, v in iterator:\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_8526/219271945.py\", line 37, in <lambda>\n","  File \"/tmp/ipykernel_8526/219271945.py\", line 12, in token2bucket_id\n","  File \"/tmp/ipykernel_8526/1607138781.py\", line 22, in _hash\n","TypeError: encoding without a string argument\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n","\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:748)\n","\n","22/01/09 21:14:41 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 15.2 in stage 14.0 (TID 930) (cluster-9376-w-2.c.my-project1-334314.internal executor 34): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n","    out_iter = func(split_index, iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 418, in func\n","    return f(iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2234, in combine\n","    merger.mergeValues(iterator)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n","    for k, v in iterator:\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_8526/219271945.py\", line 37, in <lambda>\n","  File \"/tmp/ipykernel_8526/219271945.py\", line 12, in token2bucket_id\n","  File \"/tmp/ipykernel_8526/1607138781.py\", line 22, in _hash\n","TypeError: encoding without a string argument\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n","\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:748)\n","\n","22/01/09 21:15:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 13.3 in stage 14.0 (TID 936) (cluster-9376-w-1.c.my-project1-334314.internal executor 33): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n","    out_iter = func(split_index, iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 418, in func\n","    return f(iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2234, in combine\n","    merger.mergeValues(iterator)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n","    for k, v in iterator:\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_8526/219271945.py\", line 37, in <lambda>\n","  File \"/tmp/ipykernel_8526/219271945.py\", line 12, in token2bucket_id\n","  File \"/tmp/ipykernel_8526/1607138781.py\", line 22, in _hash\n","TypeError: encoding without a string argument\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n","\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:748)\n","\n","22/01/09 21:15:21 ERROR org.apache.spark.scheduler.TaskSetManager: Task 13 in stage 14.0 failed 4 times; aborting job\n","22/01/09 21:15:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 15.3 in stage 14.0 (TID 947) (cluster-9376-w-1.c.my-project1-334314.internal executor 33): TaskKilled (Stage cancelled)\n","22/01/09 21:15:21 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 10.3 in stage 14.0 (TID 937) (cluster-9376-w-1.c.my-project1-334314.internal executor 33): TaskKilled (Stage cancelled)\n","22/01/09 21:15:22 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 3.3 in stage 14.0 (TID 938) (cluster-9376-w-1.c.my-project1-334314.internal executor 32): TaskKilled (Stage cancelled)\n"]},{"ename":"Py4JJavaError","evalue":"An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 13 in stage 14.0 failed 4 times, most recent failure: Lost task 13.3 in stage 14.0 (TID 936) (cluster-9376-w-1.c.my-project1-334314.internal executor 33): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 418, in func\n    return f(iterator)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2234, in combine\n    merger.mergeValues(iterator)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_8526/219271945.py\", line 37, in <lambda>\n  File \"/tmp/ipykernel_8526/219271945.py\", line 12, in token2bucket_id\n  File \"/tmp/ipykernel_8526/1607138781.py\", line 22, in _hash\nTypeError: encoding without a string argument\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2259)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2208)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2207)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2446)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2388)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2377)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2204)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2225)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2244)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2269)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 418, in func\n    return f(iterator)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2234, in combine\n    merger.mergeValues(iterator)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_8526/219271945.py\", line 37, in <lambda>\n  File \"/tmp/ipykernel_8526/219271945.py\", line 12, in token2bucket_id\n  File \"/tmp/ipykernel_8526/1607138781.py\", line 22, in _hash\nTypeError: encoding without a string argument\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n","output_type":"error","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)","\u001B[0;32m/tmp/ipykernel_8526/2352035203.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0mw2df_dict\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mw2df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollectAsMap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;31m# partition posting lists and write out\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpartition_postings_and_write\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpostings_filtered\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m \u001B[0mindex_const_time\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mt_start\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mcollect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    947\u001B[0m         \"\"\"\n\u001B[1;32m    948\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mSCCallSiteSync\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mcss\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 949\u001B[0;31m             \u001B[0msock_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollectAndServe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    950\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_load_from_socket\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msock_info\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd_deserializer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    951\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    109\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    110\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 111\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    112\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    113\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n","\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 13 in stage 14.0 failed 4 times, most recent failure: Lost task 13.3 in stage 14.0 (TID 936) (cluster-9376-w-1.c.my-project1-334314.internal executor 33): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 418, in func\n    return f(iterator)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2234, in combine\n    merger.mergeValues(iterator)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_8526/219271945.py\", line 37, in <lambda>\n  File \"/tmp/ipykernel_8526/219271945.py\", line 12, in token2bucket_id\n  File \"/tmp/ipykernel_8526/1607138781.py\", line 22, in _hash\nTypeError: encoding without a string argument\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2259)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2208)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2207)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2446)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2388)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2377)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2204)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2225)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2244)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2269)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 418, in func\n    return f(iterator)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2234, in combine\n    merger.mergeValues(iterator)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_8526/219271945.py\", line 37, in <lambda>\n  File \"/tmp/ipykernel_8526/219271945.py\", line 12, in token2bucket_id\n  File \"/tmp/ipykernel_8526/1607138781.py\", line 22, in _hash\nTypeError: encoding without a string argument\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 14:>                                                      (0 + 13) / 124]\r"]}],"source":["# time the index creation time\n","t_start = time()\n","# word counts map\n","word_counts = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n","postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","# filtering postings and calculate df\n","postings_filtered = postings.filter(lambda x: len(x[1])>50)\n","w2df = calculate_df(postings_filtered)\n","w2df_dict = w2df.collectAsMap()\n","# partition posting lists and write out\n","_ = partition_postings_and_write(postings_filtered).collect()\n","index_const_time = time() - t_start"]},{"cell_type":"code","execution_count":null,"id":"e6bfc29e","metadata":{"id":"e6bfc29e","nbgrader":{"grade":true,"grade_id":"collect-posting","locked":true,"points":0,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# collect all posting lists locations into one super-set\n","super_posting_locs = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix='postings_gcp_2gram_body'):\n","  if not blob.name.endswith(\"pickle\"):\n","    continue\n","  with blob.open(\"rb\") as f:\n","    posting_locs = pickle.load(f)\n","    for k, v in posting_locs.items():\n","      super_posting_locs[k].extend(v)"]},{"cell_type":"markdown","id":"463b83df","metadata":{"id":"463b83df"},"source":["Putting it all together"]},{"cell_type":"code","execution_count":null,"id":"91c5fbb4","metadata":{"id":"91c5fbb4","outputId":"d4ff0345-a329-4b0a-c28a-06c3bde4acb4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying file://index.pkl [Content-Type=application/octet-stream]...\n","/ [1 files][737.0 KiB/737.0 KiB]                                                \n","Operation completed over 1 objects/737.0 KiB.                                    \n"]}],"source":["# Create inverted index instance\n","inverted = InvertedIndex()\n","# Adding the posting locations dictionary to the inverted index\n","inverted.posting_locs = super_posting_locs\n","# Add the token - df dictionary to the inverted index\n","inverted.df = w2df_dict\n","# write the global stats out\n","inverted.write_index('.', 'index')\n","# upload to gs\n","index_src = \"index.pkl\"\n","index_dst = f'gs://{bucket_name}/postings_gcp_2gram_body/{index_src}'\n","!gsutil cp $index_src $index_dst"]},{"cell_type":"code","execution_count":null,"id":"c9048c7d","metadata":{"id":"c9048c7d","nbgrader":{"grade":false,"grade_id":"cell-index_dst_size","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"094e83de-1c74-43c7-8eaf-6db2f22dc63b"},"outputs":[{"name":"stdout","output_type":"stream","text":["736.95 KiB  2021-12-31T09:53:41Z  gs://316399401/postings_gcp_title/index.pkl\r\n","TOTAL: 1 objects, 754640 bytes (736.95 KiB)\r\n"]}],"source":["!gsutil ls -lh $index_dst"]},{"cell_type":"code","execution_count":null,"id":"4a2e1895","metadata":{"id":"4a2e1895","nbgrader":{"grade":false,"grade_id":"cell-size_ofi_input_data","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"54595c29-4ae3-4b78-86d0-d8457ae9c150"},"outputs":[{"name":"stdout","output_type":"stream","text":["14.28 GiB    gs://wikidata_preprocessed\r\n"]}],"source":["# size of input data\n","!gsutil du -sh \"gs://wikidata_preprocessed/\""]},{"cell_type":"code","execution_count":null,"id":"46087ca1","metadata":{"id":"46087ca1","nbgrader":{"grade":false,"grade_id":"cell-size_of_index_data","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"44d9721a-1cd7-4e59-9f78-5439864cfdad"},"outputs":[{"name":"stdout","output_type":"stream","text":["44.77 MiB    gs://316399401/postings_gcp_title\r\n"]}],"source":["# size of index data\n","index_dst = f'gs://316399401/postings_gcp_2gram_body/'\n","!gsutil du -sh \"$index_dst\""]},{"cell_type":"code","execution_count":null,"id":"e84c9bd0","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["22/01/09 21:15:22 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 16.2 in stage 14.0 (TID 935) (cluster-9376-w-1.c.my-project1-334314.internal executor 32): TaskKilled (Stage cancelled)\n","22/01/09 21:15:23 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.3 in stage 14.0 (TID 932) (cluster-9376-w-0.c.my-project1-334314.internal executor 25): TaskKilled (Stage cancelled)\n","22/01/09 21:15:23 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 12.3 in stage 14.0 (TID 946) (cluster-9376-w-2.c.my-project1-334314.internal executor 34): TaskKilled (Stage cancelled)\n","22/01/09 21:15:24 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 5.3 in stage 14.0 (TID 940) (cluster-9376-w-3.c.my-project1-334314.internal executor 27): TaskKilled (Stage cancelled)\n","22/01/09 21:15:24 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 14.2 in stage 14.0 (TID 933) (cluster-9376-w-0.c.my-project1-334314.internal executor 26): TaskKilled (Stage cancelled)\n","22/01/09 21:15:24 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.2 in stage 14.0 (TID 931) (cluster-9376-w-0.c.my-project1-334314.internal executor 25): TaskKilled (Stage cancelled)\n","22/01/09 21:15:24 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 8.3 in stage 14.0 (TID 934) (cluster-9376-w-0.c.my-project1-334314.internal executor 26): TaskKilled (Stage cancelled)\n","22/01/09 21:15:24 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 7.3 in stage 14.0 (TID 939) (cluster-9376-w-3.c.my-project1-334314.internal executor 27): TaskKilled (Stage cancelled)\n","22/01/09 21:15:24 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 6.3 in stage 14.0 (TID 945) (cluster-9376-w-2.c.my-project1-334314.internal executor 30): TaskKilled (Stage cancelled)\n","22/01/09 21:15:24 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 11.3 in stage 14.0 (TID 941) (cluster-9376-w-3.c.my-project1-334314.internal executor 28): TaskKilled (Stage cancelled)\n","22/01/09 21:15:25 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2.3 in stage 14.0 (TID 944) (cluster-9376-w-2.c.my-project1-334314.internal executor 34): TaskKilled (Stage cancelled)\n","22/01/09 21:15:25 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 9.3 in stage 14.0 (TID 942) (cluster-9376-w-3.c.my-project1-334314.internal executor 28): TaskKilled (Stage cancelled)\n","22/01/09 21:15:25 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 4.3 in stage 14.0 (TID 943) (cluster-9376-w-2.c.my-project1-334314.internal executor 30): TaskKilled (Stage cancelled)\n"]}],"source":["print(\"finished\")"]},{"cell_type":"code","execution_count":null,"id":"2daf425e","metadata":{},"outputs":[],"source":[]}],"metadata":{"celltoolbar":"Create Assignment","colab":{"collapsed_sections":[],"name":"index_creation_title.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":5}